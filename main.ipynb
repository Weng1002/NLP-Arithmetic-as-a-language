{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM-arithmetic\n",
    "\n",
    "## Dataset\n",
    "- [Arithmetic dataset](https://drive.google.com/file/d/1cMuL3hF9jefka9RyF4gEBIGGeFGZYHE-/view?usp=sharing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install seaborn\n",
    "!pip install opencc\n",
    "!pip install -U scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install seaborn\n",
    "# ! pip install opencc\n",
    "# ! pip install -U scikit-learn\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.utils.rnn\n",
    "import torch.utils.data\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import opencc\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data_path = './data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>src</th>\n",
       "      <th>tgt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14*(43+20)=</td>\n",
       "      <td>882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(6+1)*5=</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13+32+29=</td>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>31*(3-11)=</td>\n",
       "      <td>-248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>24*49+1=</td>\n",
       "      <td>1177</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           src   tgt\n",
       "0  14*(43+20)=   882\n",
       "1     (6+1)*5=    35\n",
       "2    13+32+29=    74\n",
       "3   31*(3-11)=  -248\n",
       "4     24*49+1=  1177"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_csv(os.path.join(data_path, 'arithmetic_train.csv'))\n",
    "df_eval = pd.read_csv(os.path.join(data_path, 'arithmetic_eval.csv'))\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform the input data to string\n",
    "df_train['tgt'] = df_train['tgt'].apply(lambda x: str(x))\n",
    "df_train['src'] = df_train['src'] + df_train['tgt'] \n",
    "df_train['len'] = df_train['src'].apply(lambda x: len(x))\n",
    "\n",
    "df_eval['tgt'] = df_eval['tgt'].apply(lambda x: str(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Dictionary\n",
    " - The model cannot perform calculations directly with plain text.\n",
    " - Convert all text (numbers/symbols) into numerical representations.\n",
    " - Special tokens\n",
    "    - '&lt;pad&gt;'\n",
    "        - Each sentence within a batch may have different lengths.\n",
    "        - The length is padded with '&lt;pad&gt;' to match the longest sentence in the batch.\n",
    "    - '&lt;eos&gt;'\n",
    "        - Specifies the end of the generated sequence.\n",
    "        - Without '&lt;eos&gt;', the model will not know when to stop generating.\n",
    "\n",
    "建立一個將「字元 → 數值ID」與「ID → 字元」的字典。\n",
    "\n",
    "（把文字（字元）轉成模型可以理解的「數字 ID」，）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 18\n",
      "\n",
      "Example:\n",
      "char_to_id: {'<pad>': 0, '<eos>': 1, '(': 2, ')': 3, '*': 4, '+': 5, '-': 6, '0': 7, '1': 8, '2': 9}\n",
      "id_to_char: {0: '<pad>', 1: '<eos>', 2: '(', 3: ')', 4: '*', 5: '+', 6: '-', 7: '0', 8: '1', 9: '2'}\n"
     ]
    }
   ],
   "source": [
    "char_to_id = {}\n",
    "id_to_char = {}\n",
    "\n",
    "# write your code here\n",
    "# Build a dictionary and give every token in the train dataset an id\n",
    "# The dictionary should contain <eos> and <pad>\n",
    "# char_to_id is to conver charactors to ids, while id_to_char is the opposite\n",
    "\n",
    "# 建立特殊符號\n",
    "# <pad>：補齊句子長度，讓 batch 中每筆資料一樣長。\n",
    "# <eos>：標記句子結尾（模型生成時知道要停下來）。\n",
    "special_tokens = ['<pad>', '<eos>']\n",
    "\n",
    "# 從訓練資料集中擷取所有字元\n",
    "# 將 src 欄位（包含題目與答案）中出現的所有字元放進集合去重\n",
    "all_chars = set(''.join(df_train['src'].tolist()))\n",
    "\n",
    "# 建立完整詞彙表，把 <pad> 和 <eos> 加到字元表的前面。\n",
    "vocab = special_tokens + sorted(list(all_chars))\n",
    "\n",
    "# 為每個字元分配一個唯一的數字編號。\n",
    "#    char_to_id：字元 -> 數字\n",
    "#    id_to_char：數字 -> 字元\n",
    "for i, ch in enumerate(vocab):\n",
    "    char_to_id[ch] = i\n",
    "    id_to_char[i] = ch\n",
    "    \n",
    "vocab_size = len(char_to_id)\n",
    "print('Vocab size: {}'.format(vocab_size))\n",
    "\n",
    "# 範例輸出\n",
    "print(\"\\nExample:\")\n",
    "print(\"char_to_id:\", {k: v for k, v in list(char_to_id.items())[:10]})\n",
    "print(\"id_to_char:\", {k: v for k, v in list(id_to_char.items())[:10]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    " - The data is processed into the format required for the model's input and output. (End with \\<eos\\> token)\n",
    "\n",
    "目標：把每一列 src（已包含等式與答案，如 3+5=8）轉成 模型輸入序列 與 訓練標籤序列。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_len_with_eos = 17\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>src</th>\n",
       "      <th>tgt</th>\n",
       "      <th>len</th>\n",
       "      <th>char_id_list</th>\n",
       "      <th>label_id_list</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14*(43+20)=882</td>\n",
       "      <td>882</td>\n",
       "      <td>14</td>\n",
       "      <td>[8, 11, 4, 2, 11, 10, 5, 9, 7, 3, 17, 15, 15, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 15, 15, 9, 1, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(6+1)*5=35</td>\n",
       "      <td>35</td>\n",
       "      <td>10</td>\n",
       "      <td>[2, 13, 5, 8, 3, 4, 12, 17, 10, 12, 1, 0, 0, 0...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 10, 12, 1, 0, 0, 0, 0, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13+32+29=74</td>\n",
       "      <td>74</td>\n",
       "      <td>11</td>\n",
       "      <td>[8, 10, 5, 10, 9, 5, 9, 16, 17, 14, 11, 1, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 14, 11, 1, 0, 0, 0, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>31*(3-11)=-248</td>\n",
       "      <td>-248</td>\n",
       "      <td>14</td>\n",
       "      <td>[10, 8, 4, 2, 10, 6, 8, 8, 3, 17, 6, 9, 11, 15...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 9, 11, 15, 1, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>24*49+1=1177</td>\n",
       "      <td>1177</td>\n",
       "      <td>12</td>\n",
       "      <td>[9, 11, 4, 11, 16, 5, 8, 17, 8, 8, 14, 14, 1, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 8, 8, 14, 14, 1, 0, 0, 0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              src   tgt  len  \\\n",
       "0  14*(43+20)=882   882   14   \n",
       "1      (6+1)*5=35    35   10   \n",
       "2     13+32+29=74    74   11   \n",
       "3  31*(3-11)=-248  -248   14   \n",
       "4    24*49+1=1177  1177   12   \n",
       "\n",
       "                                        char_id_list  \\\n",
       "0  [8, 11, 4, 2, 11, 10, 5, 9, 7, 3, 17, 15, 15, ...   \n",
       "1  [2, 13, 5, 8, 3, 4, 12, 17, 10, 12, 1, 0, 0, 0...   \n",
       "2  [8, 10, 5, 10, 9, 5, 9, 16, 17, 14, 11, 1, 0, ...   \n",
       "3  [10, 8, 4, 2, 10, 6, 8, 8, 3, 17, 6, 9, 11, 15...   \n",
       "4  [9, 11, 4, 11, 16, 5, 8, 17, 8, 8, 14, 14, 1, ...   \n",
       "\n",
       "                                       label_id_list  \n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 15, 15, 9, 1, 0...  \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 10, 12, 1, 0, 0, 0, 0, 0...  \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 14, 11, 1, 0, 0, 0, 0...  \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 9, 11, 15, 1, 0...  \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 8, 8, 14, 14, 1, 0, 0, 0...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Data Preprocessing\n",
    "PAD_ID = char_to_id['<pad>']\n",
    "EOS_ID = char_to_id['<eos>']\n",
    "\n",
    "max_len_with_eos = int(df_train['len'].max()) + 1\n",
    "\n",
    "def encode_with_eos(s: str):\n",
    "    ids = [char_to_id[ch] for ch in s]\n",
    "    ids.append(EOS_ID)\n",
    "    return ids\n",
    "\n",
    "def build_char_ids(row):\n",
    "    ids = encode_with_eos(row['src'])\n",
    "    if len(ids) < max_len_with_eos:\n",
    "        ids = ids + [PAD_ID] * (max_len_with_eos - len(ids))\n",
    "    else:\n",
    "        ids = ids[:max_len_with_eos]\n",
    "    return ids\n",
    "\n",
    "def build_label_ids(row):\n",
    "    src_str = row['src']\n",
    "    full_ids = encode_with_eos(src_str)\n",
    "    \n",
    "    # Shift left\n",
    "    labels = full_ids[1:] + [PAD_ID]\n",
    "    \n",
    "    # 找最後一個 =\n",
    "    eq_pos = src_str.rfind('=')\n",
    "    \n",
    "    # 等號之前都設為 PAD (只訓練答案)\n",
    "    if eq_pos >= 0:\n",
    "        for i in range(eq_pos):\n",
    "            if i < len(labels):\n",
    "                labels[i] = PAD_ID\n",
    "    \n",
    "    # Padding\n",
    "    if len(labels) < max_len_with_eos:\n",
    "        labels = labels + [PAD_ID] * (max_len_with_eos - len(labels))\n",
    "    else:\n",
    "        labels = labels[:max_len_with_eos]\n",
    "    \n",
    "    return labels\n",
    "\n",
    "# 應用到資料\n",
    "df_train['char_id_list'] = df_train.apply(build_char_ids, axis=1)\n",
    "df_train['label_id_list'] = df_train.apply(build_label_ids, axis=1)\n",
    "\n",
    "print(\"max_len_with_eos =\", max_len_with_eos)\n",
    "display(df_train.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyper Parameters\n",
    "\n",
    "|Hyperparameter|Meaning|Value|\n",
    "|-|-|-|\n",
    "|`batch_size`|Number of data samples in a single batch|64|\n",
    "|`epochs`|Total number of epochs to train|10|\n",
    "|`embed_dim`|Dimension of the word embeddings|256|\n",
    "|`hidden_dim`|Dimension of the hidden state in each timestep of the LSTM|256|\n",
    "|`lr`|Learning Rate|0.001|\n",
    "|`grad_clip`|To prevent gradient explosion in RNNs, restrict the gradient range|1|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "epochs = 30\n",
    "embed_dim = 256\n",
    "hidden_dim = 256\n",
    "lr = 5e-4\n",
    "weight_decay = 1e-4\n",
    "grad_clip = 1\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Batching\n",
    "- Use `torch.utils.data.Dataset` to create a data generation tool called  `dataset`.\n",
    "- The, use `torch.utils.data.DataLoader` to randomly sample from the `dataset` and group the samples into batches.\n",
    "\n",
    "- Example: 1+2-3=0\n",
    "    - Model input: 1 + 2 - 3 = 0\n",
    "    - Model output: / / / / / 0 &lt;eos&gt;  (the '/' can be replaced with &lt;pad&gt;)\n",
    "    - The key for the model's output is that the model does not need to predict the next character of the previous part. What matters is that once the model sees '=', it should start generating the answer, which is '0'. After generating the answer, it should also generate&lt;eos&gt;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, sequences):\n",
    "        self.sequences = sequences\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        # 取出第 index 筆資料的 x、y\n",
    "        if isinstance(self.sequences, (list, tuple)):\n",
    "            x_ids, y_ids = self.sequences[index]\n",
    "        else:\n",
    "            row = self.sequences.iloc[index]\n",
    "            x_ids, y_ids = row['char_id_list'], row['label_id_list']\n",
    "        return x_ids, y_ids\n",
    "\n",
    "\n",
    "# collate function, used to build dataloader\n",
    "# 把不同長度序列打包成 batch，並做 padding\n",
    "def collate_fn(batch):\n",
    "    # batch 是 list，每個元素是 (x_ids, y_ids)\n",
    "    batch_x = [torch.tensor(x, dtype=torch.long) for x, _ in batch]\n",
    "    batch_y = [torch.tensor(y, dtype=torch.long) for _, y in batch]\n",
    "\n",
    "    # 記錄原始長度（可用於 RNN pack/pad 或 mask）\n",
    "    batch_x_lens = torch.LongTensor([len(x) for x in batch_x])\n",
    "    batch_y_lens = torch.LongTensor([len(y) for y in batch_y])\n",
    "\n",
    "    # 右側用 <pad> 補齊到同長\n",
    "    pad_batch_x = torch.nn.utils.rnn.pad_sequence(\n",
    "        batch_x, batch_first=True, padding_value=char_to_id['<pad>']\n",
    "    )\n",
    "    pad_batch_y = torch.nn.utils.rnn.pad_sequence(\n",
    "        batch_y, batch_first=True, padding_value=char_to_id['<pad>']\n",
    "    )\n",
    "    \n",
    "    return pad_batch_x, pad_batch_y, batch_x_lens, batch_y_lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eval['char_id_list']  = df_eval.apply(build_char_ids,  axis=1)\n",
    "df_eval['label_id_list'] = df_eval.apply(build_label_ids, axis=1)\n",
    "\n",
    "ds_train = Dataset(df_train[['char_id_list', 'label_id_list']])\n",
    "ds_eval  = Dataset(df_eval [['char_id_list', 'label_id_list']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 17]) torch.Size([64, 17]) torch.Size([64]) torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "dl_train = torch.utils.data.DataLoader(\n",
    "    ds_train,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,          # 訓練要打亂\n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=2,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "dl_eval = torch.utils.data.DataLoader(\n",
    "    ds_eval,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,        \n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=2,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "xb, yb, xlen, ylen = next(iter(dl_train))\n",
    "print(xb.shape, yb.shape, xlen.shape, ylen.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Design\n",
    "\n",
    "## Execution Flow\n",
    "1. Convert all characters in the sentence into embeddings.\n",
    "2. Pass the embeddings through an LSTM sequentially.\n",
    "3. The output of the LSTM is passed into another LSTM, and additional layers can be added.\n",
    "4. The output from all time steps of the final LSTM is passed through a Fully Connected layer.\n",
    "5. The character corresponding to the maximum value across all output dimensions is selected as the next character.\n",
    "\n",
    "## Loss Function\n",
    "Since this is a classification task, Cross Entropy is used as the loss function.\n",
    "\n",
    "## Gradient Update\n",
    "Adam algorithm is used for gradient updates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, dropout=0.2):\n",
    "        super(CharRNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=char_to_id['<pad>'])\n",
    "\n",
    "        # 兩層 LSTM\n",
    "        self.rnn_layer1 = nn.LSTM(input_size=embed_dim, hidden_size=hidden_dim, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout)                 \n",
    "        self.rnn_layer2 = nn.LSTM(input_size=hidden_dim, hidden_size=hidden_dim, batch_first=True)\n",
    "\n",
    "        self.proj = nn.Linear(hidden_dim, vocab_size, bias=False)\n",
    "        self.proj.weight = self.embedding.weight\n",
    "        \n",
    "    def forward(self, batch_x, batch_x_lens):\n",
    "        return self.encoder(batch_x, batch_x_lens)\n",
    "    \n",
    "    # 編碼器：嵌入 → pack → LSTM×2 → pad → Linear → logits\n",
    "    def encoder(self, batch_x, batch_x_lens):\n",
    "        x = self.embedding(batch_x)                                  \n",
    "        x = nn.utils.rnn.pack_padded_sequence(x, batch_x_lens.cpu(),\n",
    "                                              batch_first=True, enforce_sorted=False)\n",
    "        x, _ = self.rnn_layer1(x)\n",
    "        x, _ = nn.utils.rnn.pad_packed_sequence(x, batch_first=True)  \n",
    "\n",
    "        x = self.dropout(x)                                         \n",
    "\n",
    "        x = nn.utils.rnn.pack_padded_sequence(x, batch_x_lens.cpu(),\n",
    "                                              batch_first=True, enforce_sorted=False)\n",
    "        x, _ = self.rnn_layer2(x)\n",
    "        x, _ = nn.utils.rnn.pad_packed_sequence(x, batch_first=True)  \n",
    "\n",
    "        logits = self.proj(x)                                       \n",
    "        return logits\n",
    "    \n",
    "    \n",
    "    def generator(self, start_char, max_len=200):\n",
    "        self.eval()\n",
    "        PAD_ID = char_to_id['<pad>']\n",
    "        EOS_ID = char_to_id['<eos>']\n",
    "        \n",
    "        # 將起始字串轉為 id\n",
    "        char_list = [char_to_id[c] for c in start_char]\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # 初始化 LSTM 狀態\n",
    "            h1, c1 = None, None\n",
    "            h2, c2 = None, None\n",
    "            \n",
    "            # 第一次:處理整個起始序列\n",
    "            x = torch.tensor([char_list], dtype=torch.long, device=device)\n",
    "            x_lens = torch.tensor([len(char_list)], dtype=torch.long, device=device)\n",
    "            \n",
    "            # 通過嵌入層\n",
    "            x_emb = self.embedding(x)\n",
    "            \n",
    "            # 第一層 LSTM\n",
    "            out1, (h1, c1) = self.rnn_layer1(x_emb)\n",
    "            out1 = self.dropout(out1)\n",
    "            \n",
    "            # 第二層 LSTM\n",
    "            out2, (h2, c2) = self.rnn_layer2(out1)\n",
    "            \n",
    "            # 從最後一個時間步開始生成\n",
    "            while len(char_list) < max_len:\n",
    "                # 投影到詞彙表\n",
    "                logits = self.proj(out2[:, -1:, :])  # 取最後一個時間步\n",
    "                next_char_id = int(torch.argmax(logits[0, 0, :]).item())\n",
    "                \n",
    "                if next_char_id == EOS_ID:\n",
    "                    break\n",
    "                \n",
    "                char_list.append(next_char_id)\n",
    "                \n",
    "                # 用新生成的字元繼續\n",
    "                x_next = torch.tensor([[next_char_id]], dtype=torch.long, device=device)\n",
    "                x_emb_next = self.embedding(x_next)\n",
    "                \n",
    "                # 通過 LSTM\n",
    "                out1, (h1, c1) = self.rnn_layer1(x_emb_next, (h1, c1))\n",
    "                out1 = self.dropout(out1)\n",
    "                out2, (h2, c2) = self.rnn_layer2(out1, (h2, c2))\n",
    "        \n",
    "        return [id_to_char[ch_id] for ch_id in char_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(4321)\n",
    "# model = CharRNN(vocab_size, embed_dim, hidden_dim).to(device)\n",
    "\n",
    "model = CharGRU(vocab_size, embed_dim, hidden_dim, dropout=0.2).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(\n",
    "    ignore_index=char_to_id['<pad>'],\n",
    "    label_smoothing=0.1,           \n",
    ")\n",
    "\n",
    "optimizer = optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=lr,\n",
    "    weight_decay=weight_decay\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "1. The outer `for` loop controls the `epoch`\n",
    "    1. The inner `for` loop uses `data_loader` to retrieve batches.\n",
    "        1. Pass the batch to the `model` for training.\n",
    "        2. Compare the predicted results `batch_pred_y` with the true labels `batch_y` using Cross Entropy to calculate the loss `loss`\n",
    "        3. Use `loss.backward` to automatically compute the gradients.\n",
    "        4. Use `torch.nn.utils.clip_grad_value_` to limit the gradient values between `-grad_clip` &lt; and &lt; `grad_clip`.\n",
    "        5. Use `optimizer.step()` to update the model (backpropagation).\n",
    "2.  After every `1000` batches, output the current loss to monitor whether it is converging.\n",
    "\n",
    "\n",
    "- Teacher Forcing：訓練時把「完整正解序列（含等式與答案）」當作輸入，\n",
    "\n",
    "    loss 只在 label_id_list 非 <pad> 的位置（=答案區段 + <eos>）計算，\n",
    "\n",
    "    等同「下一步的條件使用正確前綴」，這就是 teacher forcing。\n",
    "\n",
    "- 忽略 <pad>：criterion = CrossEntropyLoss(ignore_index=PAD_ID) 已處理，所以攤平成一維後直接丟給 CE 即可。\n",
    "\n",
    "- Exact Match (EM)：評估時必須使用 model.generator()，以「等號左邊 + '='」為起點生成；取等號右邊的整段字串與 tgt 完全相同才算 1 分。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train epoch 1: 100%|██████████| 37020/37020 [04:43<00:00, 130.76it/s, loss=0.979]\n",
      "Validation epoch 1: 100%|██████████| 263250/263250 [15:14<00:00, 287.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Train Loss: 1.1632 | EM: 0.5103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train epoch 2: 100%|██████████| 37020/37020 [04:43<00:00, 130.69it/s, loss=0.802]\n",
      "Validation epoch 2: 100%|██████████| 263250/263250 [15:20<00:00, 286.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 | Train Loss: 0.9082 | EM: 0.6268\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train epoch 3: 100%|██████████| 37020/37020 [04:43<00:00, 130.55it/s, loss=0.784]\n",
      "Validation epoch 3: 100%|██████████| 263250/263250 [15:22<00:00, 285.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 | Train Loss: 0.8346 | EM: 0.7376\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train epoch 4: 100%|██████████| 37020/37020 [04:45<00:00, 129.60it/s, loss=0.73] \n",
      "Validation epoch 4: 100%|██████████| 263250/263250 [15:35<00:00, 281.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 | Train Loss: 0.7705 | EM: 0.7879\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train epoch 5: 100%|██████████| 37020/37020 [04:43<00:00, 130.41it/s, loss=0.775]\n",
      "Validation epoch 5: 100%|██████████| 263250/263250 [15:12<00:00, 288.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 | Train Loss: 0.7386 | EM: 0.8277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train epoch 6: 100%|██████████| 37020/37020 [04:51<00:00, 126.86it/s, loss=0.743]\n",
      "Validation epoch 6: 100%|██████████| 263250/263250 [15:18<00:00, 286.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 | Train Loss: 0.7200 | EM: 0.8384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train epoch 7: 100%|██████████| 37020/37020 [04:45<00:00, 129.52it/s, loss=0.718]\n",
      "Validation epoch 7: 100%|██████████| 263250/263250 [15:04<00:00, 291.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 | Train Loss: 0.7082 | EM: 0.8615\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train epoch 8: 100%|██████████| 37020/37020 [04:44<00:00, 130.14it/s, loss=0.714]\n",
      "Validation epoch 8: 100%|██████████| 263250/263250 [15:06<00:00, 290.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 | Train Loss: 0.6997 | EM: 0.8623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train epoch 9: 100%|██████████| 37020/37020 [04:40<00:00, 131.85it/s, loss=0.701]\n",
      "Validation epoch 9: 100%|██████████| 263250/263250 [15:05<00:00, 290.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 | Train Loss: 0.6932 | EM: 0.8763\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train epoch 10: 100%|██████████| 37020/37020 [04:35<00:00, 134.40it/s, loss=0.745]\n",
      "Validation epoch 10: 100%|██████████| 263250/263250 [14:53<00:00, 294.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 | Train Loss: 0.6876 | EM: 0.8856\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train epoch 11: 100%|██████████| 37020/37020 [04:45<00:00, 129.48it/s, loss=0.653]\n",
      "Validation epoch 11: 100%|██████████| 263250/263250 [14:56<00:00, 293.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 | Train Loss: 0.6834 | EM: 0.8602\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train epoch 12: 100%|██████████| 37020/37020 [04:43<00:00, 130.46it/s, loss=0.676]\n",
      "Validation epoch 12: 100%|██████████| 263250/263250 [15:09<00:00, 289.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 | Train Loss: 0.6802 | EM: 0.8898\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train epoch 13: 100%|██████████| 37020/37020 [04:47<00:00, 128.69it/s, loss=0.687]\n",
      "Validation epoch 13: 100%|██████████| 263250/263250 [15:16<00:00, 287.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 | Train Loss: 0.6772 | EM: 0.9060\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train epoch 14: 100%|██████████| 37020/37020 [04:41<00:00, 131.39it/s, loss=0.664]\n",
      "Validation epoch 14: 100%|██████████| 263250/263250 [15:12<00:00, 288.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 | Train Loss: 0.6739 | EM: 0.8895\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train epoch 15: 100%|██████████| 37020/37020 [04:42<00:00, 131.03it/s, loss=0.65] \n",
      "Validation epoch 15: 100%|██████████| 263250/263250 [15:24<00:00, 284.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15 | Train Loss: 0.6713 | EM: 0.8972\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train epoch 16: 100%|██████████| 37020/37020 [04:43<00:00, 130.73it/s, loss=0.723]\n",
      "Validation epoch 16: 100%|██████████| 263250/263250 [15:28<00:00, 283.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16 | Train Loss: 0.6691 | EM: 0.9099\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train epoch 17: 100%|██████████| 37020/37020 [04:48<00:00, 128.44it/s, loss=0.641]\n",
      "Validation epoch 17: 100%|██████████| 263250/263250 [15:25<00:00, 284.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17 | Train Loss: 0.6669 | EM: 0.9059\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train epoch 18: 100%|██████████| 37020/37020 [04:46<00:00, 129.35it/s, loss=0.671]\n",
      "Validation epoch 18: 100%|██████████| 263250/263250 [15:21<00:00, 285.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18 | Train Loss: 0.6647 | EM: 0.9155\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train epoch 19: 100%|██████████| 37020/37020 [04:42<00:00, 131.25it/s, loss=0.69] \n",
      "Validation epoch 19: 100%|██████████| 263250/263250 [15:20<00:00, 285.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19 | Train Loss: 0.6630 | EM: 0.9123\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train epoch 20: 100%|██████████| 37020/37020 [04:43<00:00, 130.73it/s, loss=0.698]\n",
      "Validation epoch 20: 100%|██████████| 263250/263250 [15:03<00:00, 291.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20 | Train Loss: 0.6617 | EM: 0.9023\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train epoch 21: 100%|██████████| 37020/37020 [04:43<00:00, 130.62it/s, loss=0.668]\n",
      "Validation epoch 21: 100%|██████████| 263250/263250 [15:26<00:00, 284.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21 | Train Loss: 0.6603 | EM: 0.9173\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train epoch 22: 100%|██████████| 37020/37020 [04:44<00:00, 130.17it/s, loss=0.639]\n",
      "Validation epoch 22: 100%|██████████| 263250/263250 [15:19<00:00, 286.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22 | Train Loss: 0.6592 | EM: 0.9183\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train epoch 23: 100%|██████████| 37020/37020 [04:49<00:00, 128.07it/s, loss=0.621]\n",
      "Validation epoch 23: 100%|██████████| 263250/263250 [14:45<00:00, 297.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23 | Train Loss: 0.6581 | EM: 0.9141\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train epoch 24: 100%|██████████| 37020/37020 [04:41<00:00, 131.72it/s, loss=0.662]\n",
      "Validation epoch 24: 100%|██████████| 263250/263250 [15:20<00:00, 285.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24 | Train Loss: 0.6568 | EM: 0.9307\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train epoch 25: 100%|██████████| 37020/37020 [04:42<00:00, 130.99it/s, loss=0.684]\n",
      "Validation epoch 25: 100%|██████████| 263250/263250 [15:02<00:00, 291.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25 | Train Loss: 0.6565 | EM: 0.9254\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train epoch 26: 100%|██████████| 37020/37020 [04:42<00:00, 131.19it/s, loss=0.751]\n",
      "Validation epoch 26: 100%|██████████| 263250/263250 [14:57<00:00, 293.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26 | Train Loss: 0.6553 | EM: 0.9210\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train epoch 27: 100%|██████████| 37020/37020 [04:44<00:00, 130.14it/s, loss=0.616]\n",
      "Validation epoch 27: 100%|██████████| 263250/263250 [14:59<00:00, 292.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27 | Train Loss: 0.6549 | EM: 0.9306\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train epoch 28:   4%|▎         | 1377/37020 [00:10<04:35, 129.30it/s, loss=0.642]"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "train_losses = []\n",
    "eval_accuracies = []\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    model.train()\n",
    "    bar = tqdm(dl_train, desc=f\"Train epoch {epoch}\")\n",
    "    total_loss = 0\n",
    "    batch_count = 0\n",
    "    \n",
    "    for batch_x, batch_y, batch_x_lens, batch_y_lens in bar: \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        batch_x = batch_x.to(device)\n",
    "        batch_y = batch_y.to(device)\n",
    "        batch_x_lens = batch_x_lens.to(device)\n",
    "        \n",
    "        # Forward\n",
    "        logits = model(batch_x, batch_x_lens) \n",
    "        B, T, V = logits.size()\n",
    "        \n",
    "        # 計算 loss\n",
    "        loss = criterion(logits.reshape(B * T, V), batch_y.reshape(B * T))\n",
    "        \n",
    "        # Backward\n",
    "        loss.backward()\n",
    "        \n",
    "        # # 梯度裁剪\n",
    "        # if grad_clip is not None and grad_clip > 0:\n",
    "        #     torch.nn.utils.clip_grad_value_(model.parameters(), grad_clip)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        batch_count += 1\n",
    "        bar.set_postfix(loss=loss.item())\n",
    "    \n",
    "    avg_train_loss = total_loss / batch_count\n",
    "    train_losses.append(avg_train_loss)\n",
    "    \n",
    "    model.eval()\n",
    "    matched = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        bar_eval = tqdm(df_eval.iterrows(), total=len(df_eval), desc=f\"Validation epoch {epoch}\")\n",
    "        for _, row in bar_eval:\n",
    "            start_str = row['src'].split('=')[0] + '='\n",
    "            \n",
    "            # 生成答案\n",
    "            pred_chars = model.generator(start_str, max_len=max_len_with_eos)\n",
    "            pred_str = ''.join(pred_chars)\n",
    "            \n",
    "            # 提取答案部分 (最後一個等號之後)\n",
    "            if '=' in pred_str:\n",
    "                pred_ans = pred_str.split('=')[-1]\n",
    "            else:\n",
    "                pred_ans = \"\"\n",
    "            \n",
    "            gold_ans = str(row['tgt'])\n",
    "            \n",
    "            # 比對答案\n",
    "            matched += int(pred_ans == gold_ans)\n",
    "            total += 1\n",
    "    \n",
    "    acc = matched / total\n",
    "    eval_accuracies.append(acc)\n",
    "    \n",
    "    print(f\"Epoch {epoch} | Train Loss: {avg_train_loss:.4f} | EM: {acc:.4f}\")\n",
    "\n",
    "# 繪製訓練曲線\n",
    "fig, ax1 = plt.subplots(figsize=(8, 5))\n",
    "\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Train Loss', color='tab:red')\n",
    "ax1.plot(range(1, epochs + 1), train_losses, color='tab:red', marker='o', label='Train Loss')\n",
    "ax1.tick_params(axis='y', labelcolor='tab:red')\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "ax2.set_ylabel('Evaluation Accuracy (EM)', color='tab:blue')\n",
    "ax2.plot(range(1, epochs + 1), eval_accuracies, color='tab:blue', marker='x', label='Eval Accuracy')\n",
    "ax2.tick_params(axis='y', labelcolor='tab:blue')\n",
    "\n",
    "plt.title('Training Loss & Evaluation Accuracy')\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.savefig('GRU_training_curve.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, dropout=0.2):\n",
    "        super(CharRNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=char_to_id['<pad>'])\n",
    "\n",
    "        self.rnn_layer1 = nn.RNN(input_size=embed_dim, hidden_size=hidden_dim, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout)                 \n",
    "        self.rnn_layer2 = nn.RNN(input_size=hidden_dim, hidden_size=hidden_dim, batch_first=True)\n",
    "\n",
    "        self.proj = nn.Linear(hidden_dim, vocab_size, bias=False)\n",
    "        self.proj.weight = self.embedding.weight\n",
    "        \n",
    "    def forward(self, batch_x, batch_x_lens):\n",
    "        return self.encoder(batch_x, batch_x_lens)\n",
    "    \n",
    "    def encoder(self, batch_x, batch_x_lens):\n",
    "        x = self.embedding(batch_x)                                  \n",
    "        x = nn.utils.rnn.pack_padded_sequence(x, batch_x_lens.cpu(),\n",
    "                                              batch_first=True, enforce_sorted=False)\n",
    "        x, _ = self.rnn_layer1(x)\n",
    "        x, _ = nn.utils.rnn.pad_packed_sequence(x, batch_first=True)  \n",
    "\n",
    "        x = self.dropout(x)                                         \n",
    "\n",
    "        x = nn.utils.rnn.pack_padded_sequence(x, batch_x_lens.cpu(),\n",
    "                                              batch_first=True, enforce_sorted=False)\n",
    "        x, _ = self.rnn_layer2(x)\n",
    "        x, _ = nn.utils.rnn.pad_packed_sequence(x, batch_first=True)  \n",
    "\n",
    "        logits = self.proj(x)                                       \n",
    "        return logits\n",
    "    \n",
    "    def generator(self, start_char, max_len=200):\n",
    "        self.eval()\n",
    "        PAD_ID = char_to_id['<pad>']\n",
    "        EOS_ID = char_to_id['<eos>']\n",
    "        \n",
    "        char_list = [char_to_id[c] for c in start_char]\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            h1 = None\n",
    "            h2 = None\n",
    "            \n",
    "            # 第一次:處理整個起始序列\n",
    "            x = torch.tensor([char_list], dtype=torch.long, device=device)\n",
    "            x_lens = torch.tensor([len(char_list)], dtype=torch.long, device=device)\n",
    "            \n",
    "            x_emb = self.embedding(x)\n",
    "            \n",
    "            # 第一層 RNN\n",
    "            out1, h1 = self.rnn_layer1(x_emb, h1)\n",
    "            out1 = self.dropout(out1)\n",
    "            \n",
    "            # 第二層 RNN\n",
    "            out2, h2 = self.rnn_layer2(out1, h2)\n",
    "            \n",
    "            # 從最後一個時間步開始生成\n",
    "            while len(char_list) < max_len:\n",
    "                # 投影到詞彙表\n",
    "                logits = self.proj(out2[:, -1:, :])  # 取最後一個時間步\n",
    "                next_char_id = int(torch.argmax(logits[0, 0, :]).item())\n",
    "                \n",
    "                if next_char_id == EOS_ID:\n",
    "                    break\n",
    "                char_list.append(next_char_id)\n",
    "                \n",
    "                # 用新生成的字元繼續\n",
    "                x_next = torch.tensor([[next_char_id]], dtype=torch.long, device=device)\n",
    "                x_emb_next = self.embedding(x_next)\n",
    "                \n",
    "                # 通過 RNN\n",
    "                out1, h1 = self.rnn_layer1(x_emb_next, h1)\n",
    "                out1 = self.dropout(out1)\n",
    "                out2, h2 = self.rnn_layer2(out1, h2)\n",
    "        \n",
    "        return [id_to_char[ch_id] for ch_id in char_list]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GRU  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class CharGRU(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, dropout=0.2):\n",
    "        super(CharGRU, self).__init__()\n",
    "        # 嵌入層（<pad> 位置自動置零）\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=char_to_id['<pad>'])\n",
    "\n",
    "        # 兩層 GRU（取代原本 LSTM）\n",
    "        self.gru1 = nn.GRU(input_size=embed_dim,  hidden_size=hidden_dim, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.gru2 = nn.GRU(input_size=hidden_dim, hidden_size=hidden_dim, batch_first=True)\n",
    "\n",
    "        # 輸出投影；做 weight tying（需 hidden_dim == embed_dim）\n",
    "        assert hidden_dim == embed_dim, \"使用權重綁定需 hidden_dim == embed_dim\"\n",
    "        self.proj = nn.Linear(hidden_dim, vocab_size, bias=False)\n",
    "        self.proj.weight = self.embedding.weight  # weight tying\n",
    "\n",
    "    def forward(self, batch_x, batch_x_lens):\n",
    "        return self.encoder(batch_x, batch_x_lens)\n",
    "\n",
    "    # 嵌入 → pack → GRU×2 → pad → Linear\n",
    "    def encoder(self, batch_x, batch_x_lens):\n",
    "        x = self.embedding(batch_x)  # [B,T,E]\n",
    "        x = nn.utils.rnn.pack_padded_sequence(x, batch_x_lens.cpu(),\n",
    "                                              batch_first=True, enforce_sorted=False)\n",
    "        x, _ = self.gru1(x)                         # _ 是最後一步的 h（[1,B,H]），但此處不需用\n",
    "        x, _ = nn.utils.rnn.pad_packed_sequence(x, batch_first=True)  # [B,T,H]\n",
    "\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = nn.utils.rnn.pack_padded_sequence(x, batch_x_lens.cpu(),\n",
    "                                              batch_first=True, enforce_sorted=False)\n",
    "        x, _ = self.gru2(x)\n",
    "        x, _ = nn.utils.rnn.pad_packed_sequence(x, batch_first=True)  # [B,T,H]\n",
    "\n",
    "        logits = self.proj(x)  # [B,T,V]\n",
    "        return logits\n",
    "\n",
    "    def generator(self, start_char, max_len=200):\n",
    "        \"\"\"\n",
    "        自回歸生成：每次把目前序列丟進編碼器，取最後一個 time step 的 logits 做 argmax。\n",
    "        也可改成 beam search；流程與 LSTM 相同，但 GRU 只有 h，沒有 (h,c)。\n",
    "        \"\"\"\n",
    "        self.eval()\n",
    "        EOS_ID = char_to_id['<eos>']\n",
    "        ids = [char_to_id[c] for c in start_char]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            while len(ids) < max_len:\n",
    "                x = torch.tensor(ids, dtype=torch.long, device=device).unsqueeze(0)  # [1,T]\n",
    "                lens = torch.tensor([x.size(1)], dtype=torch.long, device=device)\n",
    "                logits = self.encoder(x, lens)          # [1,T,V]\n",
    "                next_id = int(torch.argmax(logits[:, -1, :], dim=-1).item())\n",
    "                if next_id == EOS_ID:\n",
    "                    break\n",
    "                ids.append(next_id)\n",
    "\n",
    "        return [id_to_char[i] for i in ids]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
